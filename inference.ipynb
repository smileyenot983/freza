{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import cv2\nimport time\nimport numpy as np\nfrom glob import glob\nimport tensorflow as tf\nimport albumentations as A\nfrom numpy.random import default_rng\n\n# Limit tensorflow from taking all the GPU memory if using it\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n    except RuntimeError as e:\n        print(e)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-24T06:36:11.856887Z","iopub.execute_input":"2021-09-24T06:36:11.857168Z","iopub.status.idle":"2021-09-24T06:36:11.864675Z","shell.execute_reply.started":"2021-09-24T06:36:11.857141Z","shell.execute_reply":"2021-09-24T06:36:11.863782Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Input params\nframes_num = 8\nseq_length = 16\ninput_size = 224\n\nbackbone = '0'\nclasses = {0:'default', 1:'bienie'}\n\nDATASET_PATH = \"/kaggle/input/bienievideos\"\nWEIGHTS_PATH = \"/kaggle/input/bienieweights\"","metadata":{"execution":{"iopub.status.busy":"2021-09-24T06:26:28.206551Z","iopub.execute_input":"2021-09-24T06:26:28.207150Z","iopub.status.idle":"2021-09-24T06:26:28.211513Z","shell.execute_reply.started":"2021-09-24T06:26:28.207106Z","shell.execute_reply":"2021-09-24T06:26:28.210935Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Gather video file(s) path(s)\nvideos = glob(f\"{DATASET_PATH}/*/*.mp4\")","metadata":{"execution":{"iopub.status.busy":"2021-09-24T06:33:36.798131Z","iopub.execute_input":"2021-09-24T06:33:36.798418Z","iopub.status.idle":"2021-09-24T06:33:36.808488Z","shell.execute_reply.started":"2021-09-24T06:33:36.798392Z","shell.execute_reply":"2021-09-24T06:33:36.807450Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Class for reading video and taking frames\nclass VideoReader:\n\n    def __init__(self):\n        \n        self.rng = default_rng()\n\n    def read_random_frames_sequences(self, path, frames_num, seq_length, input_size):\n        \n        capture = cv2.VideoCapture(path)\n        frames_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n        \n        frames = {}        \n        for start_frame_idx in self.rng.choice(frames_count-seq_length, size=frames_num, replace=False):\n            frames[start_frame_idx] = []\n            for frame_idx in np.arange(start_frame_idx, start_frame_idx+seq_length):\n                frames[start_frame_idx].append(self._read_frame_at_index(path, capture, frame_idx, input_size))\n        capture.release()\n        return frames\n\n    def _read_frame_at_index(self, path, capture, frame_idx, input_size):\n        \n        capture.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n        ret, frame = capture.read()    \n        if not ret or frame is None:\n            return None\n        else:\n            return A.Resize(height=input_size, width=input_size)(image=cv2.cvtColor(frame[:,128:-128], cv2.COLOR_BGR2RGB))['image']","metadata":{"execution":{"iopub.status.busy":"2021-09-24T06:33:37.421371Z","iopub.execute_input":"2021-09-24T06:33:37.421678Z","iopub.status.idle":"2021-09-24T06:33:37.433095Z","shell.execute_reply.started":"2021-09-24T06:33:37.421650Z","shell.execute_reply":"2021-09-24T06:33:37.432046Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Model architecture\ndef efnet_lstm(backbone='0', seq_length=16, input_size=224):\n\n    if backbone == '0':\n        EFNet = tf.keras.applications.efficientnet.EfficientNetB0\n    elif backbone == '1':\n        EFNet = tf.keras.applications.efficientnet.EfficientNetB1\n    elif backbone == '2':\n        EFNet = tf.keras.applications.efficientnet.EfficientNetB2\n    elif backbone == '3':\n        EFNet = tf.keras.applications.efficientnet.EfficientNetB3\n    elif backbone == '4':\n        EFNet = tf.keras.applications.efficientnet.EfficientNetB4\n    elif backbone == '5':\n        EFNet = tf.keras.applications.efficientnet.EfficientNetB5\n    elif backbone == '6':\n        EFNet = tf.keras.applications.efficientnet.EfficientNetB6\n    elif backbone == '7':\n        EFNet = tf.keras.applications.efficientnet.EfficientNetB7\n        \n    bottleneck = EFNet(weights='imagenet', include_top=False, pooling='avg')\n    inp = tf.keras.layers.Input((seq_length, input_size, input_size, 3))\n    x = tf.keras.layers.TimeDistributed(bottleneck)(inp)\n    x = tf.keras.layers.LSTM(128)(x)\n    x = tf.keras.layers.Dense(64, activation='elu')(x)\n    x = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n\n    model = tf.keras.Model(inp,x)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2021-09-24T06:41:59.270624Z","iopub.execute_input":"2021-09-24T06:41:59.270978Z","iopub.status.idle":"2021-09-24T06:41:59.282702Z","shell.execute_reply.started":"2021-09-24T06:41:59.270949Z","shell.execute_reply":"2021-09-24T06:41:59.281773Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Initialize model and load weights\nmodel = efnet_lstm(backbone=backbone, seq_length=seq_length, input_size=input_size)\nmodel.load_weights(f\"{WEIGHTS_PATH}/best.hdf5\")","metadata":{"execution":{"iopub.status.busy":"2021-09-24T06:42:00.985803Z","iopub.execute_input":"2021-09-24T06:42:00.986116Z","iopub.status.idle":"2021-09-24T06:42:06.535585Z","shell.execute_reply.started":"2021-09-24T06:42:00.986086Z","shell.execute_reply":"2021-09-24T06:42:06.534767Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Initialize video reader and select video file\nvideo_reader = VideoReader()\nvideo = videos[0]","metadata":{"execution":{"iopub.status.busy":"2021-09-24T06:42:07.314066Z","iopub.execute_input":"2021-09-24T06:42:07.314366Z","iopub.status.idle":"2021-09-24T06:42:07.322457Z","shell.execute_reply.started":"2021-09-24T06:42:07.314335Z","shell.execute_reply":"2021-09-24T06:42:07.321746Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Inference\nprint(f\"Video file: {video}\\n\")\n\nt = time.time()\nprint(f\"Reading {frames_num} random frames sequences...\")\nframes = video_reader.read_random_frames_sequences(video, frames_num, seq_length, input_size)\nprint(f\"Reading random frames sequences starting at indices {list(frames.keys())} has been finished\\n\")\n\nprint(f\"Generating predictions...\")\npreds = []\nfor frame_idx, images in frames.items():\n    preds.append(model.predict(np.expand_dims(images, axis=0)))\n    \ngt = video.split('/')[-2]\nconf = np.array(preds).sum()/frames_num\nprobability = conf if conf>0.5 else 1-conf\npred = classes[int(conf>0.5)]\nprint(f\"The video contains {pred.upper()} scenario with {100*probability:.2f}% probability\")\nprint(f\"Ground truth: {gt.upper()}\")\nprint(f\"Inference time: {time.time()-t:.2f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2021-09-24T06:42:08.060200Z","iopub.execute_input":"2021-09-24T06:42:08.060754Z","iopub.status.idle":"2021-09-24T06:42:22.437828Z","shell.execute_reply.started":"2021-09-24T06:42:08.060719Z","shell.execute_reply":"2021-09-24T06:42:22.436789Z"},"trusted":true},"execution_count":34,"outputs":[]}]}